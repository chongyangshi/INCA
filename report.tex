\documentclass[10pt, oneside]{article}   	
\usepackage[left=20mm,top=20mm,right=20mm,bottom=20mm]{geometry}   
\geometry{a4paper}
\usepackage{algorithm,algpseudocode}  
\usepackage{graphicx}					
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{gensymb}
\usepackage{url}
\usepackage{titlesec}
\usepackage[parfill]{parskip}
\usepackage{cite}
\usepackage{array}
\usepackage{siunitx}
\usepackage{listings}
\usepackage{fixltx2e}
\usepackage{color}
\usepackage[usenames,dvipsnames,svgnames,table]{xcolor}
\usepackage[parfill]{parskip}
\setlength{\headsep}{5pt}
\graphicspath{ {images/} }
\lstset{
  basicstyle=\ttfamily,
  columns=fixed,
  fontadjust=true,
  basewidth=0.5em
}
\titleformat{\section}
{\normalfont\large\bfseries}{\thesection}{1em}{}
\titleformat{\subsection}
{\normalfont\normalsize\bfseries}{\thesubsection}{1em}{}
\title{\vspace{-1.6cm}INCA Open Examination Report}
\author{Exam Number: Y3603***}
\date{}							
\begin{document}
\maketitle
\section{Discussion of architectures} \label{sec:architectures}
The type of problem required to be solved with a neural network is a classification problem -- that is, to decide class membership of an unknown data item, based on another data set of data items with known class memberships \cite[Sec. 2]{Dreiseitl2002352}. Depending on the target output, the purpose could be to classify a set of inputs into two or more classes. This problem requires the simplest form of binary classification \cite[Fig. 4]{candanedo2016accurate}, where the output should be either \textit{yes} (room occupied) or \textit{no} (unoccupied). 

Given the nature of the problem, a wide range of feedforward architectures can be chosen. This section gives a brief discussion on the features of each architecture, and performs test-runs of that architecture's network in various configurations to find the best cursory performance. The MATLAB Neural Network Toolbox \cite{kohonen2014matlab} will be used throughout this report, unless otherwise noted.

The data used in these cursory test-runs are directly imported from the data CSVs with minimal processing. The training dataset is used to train the networks, while the two test datasets merged into one (for now) is used to test the networks on unseen data. Input and target datasets are separated due to toolbox requirements. At this stage, the only pre-processing done is the removal of time, which will be later experimented with the chosen architecture.

\subsection{Multilayer perceptron networks with backpropagation}

Multilayer perceptron (MLP) is a typical feedforward neural network. In a MLP, neurons are arranged in layers, which consist of an input layer, one or more hidden layers, and an output layer. The feedforward property means that neurons are connected from one layer to the next with no `lateral' or `feedback' connections \cite{som-lecture}, and the backpropagation property through training functions implies the network's ability to propagate errors at the output layer back through the network to update weights during training, improving performance. MLP requires supervised training -- with known target for the training dataset. MLP is useful to solve both regression and classification problems, but careful control of network configuration and training parameters is required to avoid overfitting. The selection of number of layers and layer sizes are important in creating accurate and useful MLP networks.

Three different backpropagation training functions are used for MLP: Levenberg-Marquardt ($trainlm$), gradient descent ($traingd$), and gradient descent with momentum ($traingdm$) for updating weight and bias values during training. All three are trained on the training dataset with various layer configurations to test for best performance. The performance here is measured by two indices: the mean square error (MSE) and the actual misclassification rate (with classification determined by rounding to 0 or 1), both on the testing dataset unseen during training, as in practical use the network nearly always works with unseen data. All other parameters are toolbox default. The results of the testing can be seen in Figure \ref{fig:mlp-testing}.

\begin{figure}[h]
\begin{center}
\fontsize{9}{11}\selectfont
\begin{tabular}{|c|c|c|c|c|c|}
\hline 
MLP Configuration & [5 3 2] & [6 4 2] & [10 5 5] & [20 10 10] & [6 4] \\ \hline \hline 
$trainlm$ Validation MSE & 0.077 & 0.090 & 0.062 & 0.093 & 0.045 \\ \hline 
$trainlm$ Misclassification (\%) & 7.87 & 9.63 & 6.21 & 9.87 & 8.15 \\ \hline \hline 
$traingd$ Validation MSE & 0.149 & 0.053 & 0.101 & 0.054 & 0.088 \\ \hline 
$traingd$ Misclassification (\%) & 16.5 & 4.16 & 10.2 & 6.19 & 11.8 \\ \hline \hline 
$traingdm$ Validation MSE & 0.046 & 0.197 & 0.109 & 0.033 & 0.032 \\ \hline 
$traingdm$ Misclassification (\%) & 5.87 & 27.5 & 16.9 & 2.24 & 3.31 \\ \hline 
\end{tabular}
\end{center}
\caption{\label{fig:mlp-testing} Results from cursory testing of multilayer perceptron (MLP) networks.}
\end{figure}

Barring the large [20 10 10] architecture (while the best external validation performance for $traingdm$, is slow to train and has a tendency to overfit other training data), the best performing architectures for $trainlm$, $traingd$ and $traingdm$ appear to be [10 5 5], [6 4 2] and [6 4] respectively. It is worth nothing that these are performed on data with limited preprocessing, and are indicative only to determine what configurations or similar configurations could be potentially used if MLP is chosen to be the final architecture.

\subsection{Radial basis function networks}

The radial basis function (RBF) network is another feedforward architecture with the ability to solve linearly inseparable classification problems. Rather than using hidden layers of identical neurons to achieve this like MLP, a RBF network transforms data into a higher dimension through the use of a layer of fully connected neurons computing radial basis functions \cite{rbf-lecture}. This often allows the RBF network to solve classification problems more efficiently than MLP, but at the same time makes training on large data samples a slow process, due to the need of computing large numbers of distinct basis functions \cite[p. 260]{haykin2008}.

The toolbox provides two methods for creating an RBF network: exact fit and fewer neurons. The former, while fast, is not used due to its high tendency of overfitting. The latter requires a definite goal MSE as stopping condition, thus requires much more time to train, but does not have the disadvantage of the exact fit method. Based on the MSEs seen during MLP training, four goals are used to train four different RBF networks, as shown in Figure \ref{fig:rbf-testing}. All other parameters are toolbox defaults.

\begin{figure}[h]
\begin{center}
\fontsize{9}{11}\selectfont
\begin{tabular}{|c|c|c|c|c|c|}
\hline 
RBF Training MSE Goal & 0.2 & 0.1 & 0.05 & 0.01 & 0.001 \\ \hline \hline 
Validation MSE & 0.185 & 0.197 & 0.214 & 0.234 & 0.240 \\ \hline 
Misclassification (\%) & 24.33 & 24.31 & 24.31 & 24.28 & 24.27 \\ \hline 
\end{tabular}
\end{center}
\caption{\label{fig:rbf-testing} Results from cursory testing of radial basis function (RBF) networks.}
\end{figure}

From past experience with the toolbox, training of RBF networks with the toolbox appears to be deterministic. It can be observed that RBF networks perform significantly worse than MLP networks (Figure \ref{fig:mlp-testing}) with the same input, especially in terms of actual misclassification rate on unseen data.

\subsection{Self-organising maps} \label{subsec:som-test}

Self-organising maps (SOM) is a feedforward neural network architecture often used for specialised purposes such as data visualisation. The network consists of a mesh of connected neurons that attempts to rearrange positions match a data distribution in a iterative process \cite[p. 34]{som-lecture}. While it is often used to reduce the dimension of data for visualisation purposes, it can also be used to solve classification problems \cite{owens2000application}. 

In particular, SOM is able to produce a out-dimensional output mesh from a multi-dimensional input dataset, allowing the computation of both linear regression and classification problems, as demonstrated by Haykin \cite[Sec. 9.5]{haykin2008}. This allows the binary classification problem to be solved by fitting a one-dimensional mesh to the training data, and determining which side of the mesh is a testing input placed. Further more, SOM deploys unsupervised learning and does not require targets for training data. This reduces the likelihood of bias towards patterns of training data, and allows continuous training in use.

The same training and testing datasets as used for MLP and RBF networks are used to evaluate SOM performance, with varying single-dimensional mesh sizes (also the number of neurons), as shown in Figure \ref{fig:som-testing}. Toolbox defaults such as $hexgrid$ and $linkdist$ are used.

\begin{figure}[h]
\begin{center}
\fontsize{9}{11}\selectfont
\begin{tabular}{|c|c|c|c|c|c|c|}
\hline 
SOM Mesh Size & [2 1] & [5 1] & [10 1] & [20 1] & [30 1] & [40 1] \\ \hline 
Validation MSE & 0.113 & 0.090 & 0.055 & 0.084 & 0.138 & 0.125 \\ \hline 
Misclassification (\%) & 24.3 & 11.3 & 7.33 & 3.21 & 4.83 & 3.74 \\ \hline \hline 
SOM Mesh Size & [50 1] & [60 1] & [70 1] & [80 1] & [90 1] & [100 1] \\ \hline 
Validation MSE & 0.162 & 0.138 & 0.150 & 0.166 & 0.207 & 0.225 \\ \hline 
Misclassification (\%) & 11.1 & 10.8 & 9.89 & 11.7 & 30.2 & 29.7 \\ \hline 
\end{tabular}
\end{center}
\caption{\label{fig:som-testing} Results from cursory testing of self-organising maps (SOM).}
\end{figure}

It can be observed that SOM exhibits a steady and accurate performance between 10 and 40 neurons in the mesh. The best performance with 20 neurons is comparable to those of MLP.

\subsection{Selection of the final architecture}

Based on the results from cursory testings of the three architectures discussed, as well their characteristics, the \textbf{self-organising map (SOM)} is chosen as the final architecture for further experimentation. 

RBF is firstly ruled out due to its consistently high misclassification rate observed on unseen data, making it undesirable for the purpose of the problem: accurately identify the occupancy of a room based on sensor data only. While different MLP training functions do produce comparable performance to SOM at different layer configurations, it is ultimately decided that the constant influx of new data during the operation of the occupancy identification system would create hassle in constantly sourcing data for the supervised training of MLP, as the patterns in sensor data will change as the season or other environmental conditions change. The unsupervised training of SOM allows the system to take into account new data in far less computational expensive way. Thus, MLP is also ruled out, leaving SOM as the chosen architecture.

In addition, a few more runs of the cursory testing also shows that the misclassification rates of MLP fluctuate more significantly than SOM, which could potentially cause the system to be less stable in producing accurate results if MLP is used in place of SOM. 

\section{Room occupancy detection with self-organising maps}

\subsection{Data exploration and pre-processing}
While Section \ref{sec:architectures} merged the two testing datasets for convenience, according to the source of the data, the two testing datasets are different in nature: the much smaller first dataset was recorded mostly when the door to the room is closed, and the bigger second dataset was recorded mostly with the door open \cite[Tb. 5]{candanedo2016accurate}. Therefore, for the in-depth study of the data in this section, the two testing datasets will be processed separately. The number of inputs in the training dataset and the two test datasets (Validation 1, Validation 2) are 8143, 2665 and 9752 respectively.

The training dataset will be used to train the SOM, while the two testing datasets will be used as unseen inputs to evaluate the performance of the trained SOM. There is no missing value. With the exception of time, all values are numerical. Therefore no imputation or numerical encodings are required. All three datasets exhibit a relative imbalance in class membership: the percentages of room unoccupied are 79\%, 64\% and 79\% respectively. While is could potentially cause a bias in training and testing data, it was decided that no rebalancing measures should be conducted, as (1) the training and testing datasets exhibit a common pattern, and (2) this appears to be the normal usage pattern of the room in question, as well as the fact that 79\% is not an overwhelming majority that could skew results, such as those observed in using SOM to detect transaction fraud (negative $>$99\%) \cite{almendra2014using}, which would require special measures for the SOM to function reliably. 

To further illustrate the effects of rebalancing, a comparison test was conducted between a training dataset of 1729 randomly sampled unoccupied inputs and all 1729 occupied inputs, and a training dataset of 3458 randomly sampled inputs with no particular pattern. Each side of comparison was freshly trained ten times, and the results are as shown in Figure \ref{fig:equalize-testing}, demonstrating that it is better \textit{not} to rebalance the inputs in SOM training. It is worth noting that as SOM deploys unsupervised training, the network is not aware of the class distribution of the training dataset.

\begin{figure}[h]
\begin{center}
\fontsize{9}{11}\selectfont
\begin{tabular}{|c|c|c|}
\hline 
Rebalance of Input Dataset & Yes (1729 Positive + 1729 Negative) & No (3458 Random) \\ \hline
Mean Training MSE & 0.096 & 0.055 \\ \hline \hline 
Mean Validation 1 MSE & 0.071 & 0.069 \\ \hline 
Mean Misclassification 1 (\%) & 8.96 & 2.18 \\ \hline \hline 
Mean Validation 2 MSE & 0.112 & 0.140 \\ \hline 
Mean Misclassification 2 (\%) & 16.08 & 9.23 \\ \hline
\end{tabular}
\end{center}
\caption{\label{fig:equalize-testing} Results from testing the balancing of training input on a [20 1] SOM. (All MSE variance $<0.01$, all \% variance $<0.02$.)}
\end{figure}

Two types of data based on time are provided: the sequence number and the exact time. Both of which could be potentially useful for the network, especially in studying the time-based occupation pattern of the room studied. But at the same time, this could cause the loss of generalisability, as well as affecting the network performance. In order to determine the suitability of using time, the best SOM configuration Section \ref{subsec:som-test} -- [20 1] is trained by three variations of the training dataset: time removed, with time sequence numbers, and with exact times converted into integer timestamps. In the latter two variations, the time variable is scaled to between 0 and 1. For all variations, the other five input variables are attached as-is. The results of the testing can be found in Figure \ref{fig:time-testing}.

\begin{figure}[h]
\begin{center}
\fontsize{9}{11}\selectfont
\begin{tabular}{|c|c|c|c|}
\hline 
Time Variation in Input & No Time & Integer Timestamps & Time Sequence Numbers \\ \hline \hline 
Validation 1 MSE & 0.063 & 0.063 & 0.062 \\ \hline 
Misclassification 1 (\%) & 2.18 & 2.18 & 2.18 \\ \hline \hline 
Validation 2 MSE &  0.090 & 0.089 & 0.090 \\ \hline 
Misclassification 2 (\%) & 3.51 & 3.41 & 3.48 \\ \hline 
\end{tabular}
\end{center}
\caption{\label{fig:time-testing} Results from testing the use of time in training input on a [20 1] SOM.}
\end{figure}

From the results, it can be seen that the inclusion of time (in either form) has almost no effect on the performance of network on unseen test data, confirming the assessment specification that the other input variables (from sensor data) have already included the influence of time. Therefore, for the consideration of generalisability, time is removed from the input variables.

This leaves five input variables, all based on sensor data, as shown in Figure \ref{fig:final-variables}. 

\begin{figure}[h]
\begin{center}
\fontsize{9}{11}\selectfont
\begin{tabular}{|c|c|c|c|}
\hline 
\# & Input Variable & Type & Range \\ \hline 
1 & Temperature & Numeric & [19, 24.4083] \\ \hline 
2 & Humidity & Numeric & [16.745, 39.5] \\ \hline 
3 & Light & Numeric & [0, 1697.2] \\ \hline 
4 & CO\textsubscript{2} & Numeric & [412.75, 2076.5] \\ \hline
5 & HumidityRatio & Numeric & [0.0027, 0.0065] \\ \hline
\end{tabular}
\end{center}
\caption{\label{fig:final-variables} Input variables to the SOM.}
\end{figure}



\bibliographystyle{IEEEtran}
\small{\bibliography{report}}
\end{document}  